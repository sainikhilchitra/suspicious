<h2>1. What is your project about? (Big Picture)</h2>

<p>
Your project is about video anomaly detection in surveillance videos, with a specific interest in subtle behaviors like theft.
</p>

<p>
In simple terms:
</p>

<p>
You want a system that learns what normal behavior looks like in a surveillance scene and raises an alert when something unexpected or abnormal (like theft) happens ‚Äî without needing labeled anomaly data.
</p>

<hr/>

<h2>2. Why is this problem hard?</h2>

<p>
From your literature survey, the main challenges are:
</p>

<ul>
  <li><b>Anomalies are rare</b>
    <ul>
      <li>You don‚Äôt have enough theft videos to train supervised models.</li>
    </ul>
  </li>

  <li><b>Theft is subtle</b>
    <ul>
      <li>Small hand movements, object interaction, short duration.</li>
      <li>Not big visual changes like explosions or accidents.</li>
    </ul>
  </li>

  <li><b>No clear labels</b>
    <ul>
      <li>Most real surveillance data is unlabeled.</li>
    </ul>
  </li>

  <li><b>Existing methods fail here</b>
    <ul>
      <li>Pixel-level errors detect large anomalies but miss subtle ones.</li>
      <li>Background noise causes false alarms.</li>
    </ul>
  </li>
</ul>

<hr/>

<h2>3. How does the literature solve this problem?</h2>

<p>
Most papers you studied follow this idea:
</p>

<p>
<b>üëâ Future-frame prediction paradigm</b>
</p>

<p>
<b>Core idea:</b>
</p>

<ul>
  <li>Train a model only on normal videos</li>
  <li>The model learns to predict the next frame</li>
  <li>If the future frame cannot be predicted well ‚Üí anomaly</li>
</ul>

<p>
This works because:
</p>

<ul>
  <li>Normal behavior is predictable</li>
  <li>Abnormal behavior is not</li>
</ul>

<p>
This paradigm is used in:
</p>

<ul>
  <li>Liu et al. 2018</li>
  <li>Zhu et al. 2019</li>
  <li>Memory-augmented methods</li>
  <li>Distillation methods</li>
  <li>Hybrid prediction‚Äìreconstruction methods</li>
</ul>

<p>
So you are not inventing a new paradigm ‚Äî you are building on a well-accepted one.
</p>

<hr/>

<h2>4. What is the main problem with existing methods?</h2>

<p>
From your table, the same limitations repeat again and again:
</p>

<p>
<b>‚ùå Pixel-level anomaly scoring (PSNR, MSE)</b>
</p>

<ul>
  <li>Sensitive to lighting, shadows, camera noise</li>
  <li>Treats background and action equally</li>
  <li>Misses subtle theft-like actions</li>
</ul>

<p>
<b>‚ùå No focus mechanism</b>
</p>

<ul>
  <li>Model looks at the whole frame equally</li>
  <li>Theft happens in small regions, not everywhere</li>
</ul>

<p>
<b>‚ùå Heavy solutions exist</b>
</p>

<ul>
  <li>Memory banks</li>
  <li>Diffusion models</li>
  <li>Teacher‚Äìstudent frameworks</li>
</ul>

<p>
But they are:
</p>

<ul>
  <li>Computationally expensive</li>
  <li>Hard to train</li>
  <li>Overkill for your goal</li>
</ul>

<hr/>

<h2>5. What is YOUR idea? (Core contribution)</h2>

<p>
You keep the future-frame prediction framework, but you fix its weakness.
</p>

<p>
<b>Your idea is:</b>
</p>

<p>
Add intelligence to prediction-based anomaly detection by guiding the model to focus on important spatial and temporal regions and by improving anomaly scoring beyond raw pixel error.
</p>

<p>
That‚Äôs it.<br/>
Simple, clear, strong.
</p>

<hr/>

<h2>6. What exactly are you adding? (Unique features)</h2>

<p>
You add two key things ‚Äî nothing more, nothing less.
</p>

<h3>6.1 Spatio-Temporal Attention (MOST IMPORTANT)</h3>

<p>
<b>What it is</b>
</p>

<p>
A lightweight module that tells the model:
</p>

<ul>
  <li>Where to look (spatial attention)</li>
  <li>When to care (temporal attention)</li>
</ul>

<p>
<b>Why it is needed</b>
</p>

<ul>
  <li>Theft happens in localized regions</li>
  <li>Background motion should not dominate the score</li>
</ul>

<p>
<b>What it fixes</b>
</p>

<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Problem</th>
    <th>Fixed by Attention</th>
  </tr>
  <tr>
    <td>Background noise</td>
    <td>Suppressed</td>
  </tr>
  <tr>
    <td>Subtle hand/object motion</td>
    <td>Highlighted</td>
  </tr>
  <tr>
    <td>False positives</td>
    <td>Reduced</td>
  </tr>
</table>

<p>
<b>Why this is safe and valid</b>
</p>

<ul>
  <li>Attention is already accepted in VAD literature</li>
  <li>You are using it in a new place (prediction-based framework)</li>
</ul>

<h3>6.2 Feature-Level Anomaly Scoring</h3>

<p>
<b>What it is</b>
</p>

<p>
Instead of comparing only pixels, you also compare:
</p>

<ul>
  <li>Deep feature representations of predicted vs real frames</li>
</ul>

<p>
<b>Why this matters</b>
</p>

<p>
Features encode:
</p>

<ul>
  <li>Motion patterns</li>
  <li>Semantics</li>
</ul>

<p>
Less sensitive to illumination and noise
</p>

<p>
<b>What it fixes</b>
</p>

<table border="1" cellpadding="6" cellspacing="0">
  <tr>
    <th>Problem</th>
    <th>Fixed by Feature Scoring</th>
  </tr>
  <tr>
    <td>Pixel noise</td>
    <td>Ignored</td>
  </tr>
  <tr>
    <td>Subtle behavior changes</td>
    <td>Captured</td>
  </tr>
  <tr>
    <td>PSNR weakness</td>
    <td>Reduced</td>
  </tr>
</table>

<hr/>

<h2>7. What is the full architecture? (Clear explanation)</h2>

<p>
Here is exactly what happens, step by step.
</p>

<h3>Step 1: Input frames</h3>
<ul>
  <li>Take last N frames (e.g., 4‚Äì5)</li>
  <li>This gives temporal context</li>
</ul>

<h3>Step 2: CNN Encoder</h3>
<p>
Extract spatial features:
</p>
<ul>
  <li>Objects</li>
  <li>Edges</li>
  <li>Motion cues</li>
</ul>

<p>
Why?<br/>
Pixels are noisy; features are meaningful.
</p>

<h3>Step 3: ConvLSTM</h3>
<ul>
  <li>Processes features across time</li>
  <li>Learns normal temporal evolution</li>
</ul>

<p>
Why?<br/>
Normal behavior follows predictable patterns.
</p>

<h3>Step 4: Spatio-Temporal Attention (YOUR KEY MODULE)</h3>
<ul>
  <li>Weighs ConvLSTM features</li>
  <li>Focuses on:
    <ul>
      <li>Moving objects</li>
      <li>Interaction zones</li>
      <li>Critical time steps</li>
    </ul>
  </li>
</ul>

<p>
Why?<br/>
Theft is small and localized.
</p>

<h3>Step 5: Decoder</h3>
<ul>
  <li>Predicts the next frame</li>
  <li>Uses attended features</li>
</ul>

<p>
Why?<br/>
Prediction enforces learning of normal behavior.
</p>

<h3>Step 6: Anomaly Scoring</h3>

<p>
You compute:
</p>

<ul>
  <li>Pixel-level error</li>
  <li>Feature-level discrepancy</li>
</ul>

<p>
Then combine them into a final anomaly score.
</p>

<p>
High score ‚Üí anomaly<br/>
Low score ‚Üí normal
</p>

<hr/>

<h2>8. Training vs Testing (VERY IMPORTANT)</h2>

<h3>Training</h3>
<ul>
  <li>Only normal videos</li>
  <li>No theft labels</li>
  <li>Model learns normality</li>
</ul>

<h3>Testing</h3>
<ul>
  <li>Unseen videos (may contain theft)</li>
  <li>Prediction fails when abnormal behavior appears</li>
  <li>Attention highlights abnormal regions</li>
  <li>Score spikes</li>
</ul>

<p>
This is unsupervised anomaly detection.
</p>

<hr/>

<h2>9. What you are NOT doing (important clarity)</h2>

<p>
You are not:
</p>

<ul>
  <li>Doing supervised classification</li>
  <li>Using reinforcement learning</li>
  <li>Using diffusion or CLIP</li>
  <li>Building heavy multi-task systems</li>
</ul>

<p>
This keeps your work:
</p>

<ul>
  <li>Clean</li>
  <li>Explainable</li>
  <li>Reproducible</li>
</ul>

<hr/>

<h2>10. What is your final contribution (clear & defensible)</h2>

<p>
You can confidently claim:
</p>

<ul>
  <li>An attention-enhanced future-frame prediction framework</li>
  <li>Improved anomaly scoring using feature-level discrepancy</li>
  <li>Better detection of subtle theft-like anomalies</li>
  <li>Lightweight and unsupervised design</li>
</ul>
