{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65eaf238-5ff5-4ac2-8a54-265aba184a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class UCSDPed2Dataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length=5, image_size=128, mode=\"train\"):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == \"train\":\n",
    "            self.video_dir = os.path.join(root_dir, \"train\")\n",
    "        else:\n",
    "            self.video_dir = os.path.join(root_dir, \"test\")\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.samples = []\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        video_folders = sorted(os.listdir(self.video_dir))\n",
    "\n",
    "        for video in video_folders:\n",
    "            video_path = os.path.join(self.video_dir, video)\n",
    "\n",
    "            if not os.path.isdir(video_path):\n",
    "                continue\n",
    "\n",
    "            frames = sorted([\n",
    "                f for f in os.listdir(video_path)\n",
    "                if f.lower().endswith((\".jpg\", \".png\", \".tif\"))\n",
    "            ])\n",
    "\n",
    "            for i in range(len(frames) - self.sequence_length):\n",
    "                self.samples.append((\n",
    "                    video_path,\n",
    "                    frames[i:i + self.sequence_length],\n",
    "                    frames[i + self.sequence_length]\n",
    "                ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, input_frames, target_frame = self.samples[idx]\n",
    "    \n",
    "        input_tensor = []\n",
    "        for f in input_frames:\n",
    "            img = Image.open(os.path.join(video_path, f)).convert(\"L\")\n",
    "            img = self.transform(img)\n",
    "            input_tensor.append(img)\n",
    "    \n",
    "        input_tensor = torch.stack(input_tensor, dim=0)\n",
    "    \n",
    "        target_img = Image.open(\n",
    "            os.path.join(video_path, target_frame)\n",
    "        ).convert(\"L\")\n",
    "        target_img = self.transform(target_img)\n",
    "    \n",
    "        video_name = os.path.basename(video_path)  # e.g., \"Test001\" or \"Test002\"\n",
    "    \n",
    "        return input_tensor, target_img, video_name, target_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce06f71a-fe37-4514-b5a8-7e2abb35367f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv = nn.Conv2d(\n",
    "            input_dim + hidden_dim,\n",
    "            4 * hidden_dim,\n",
    "            3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        i, f, o, g = torch.split(self.conv(combined), self.hidden_dim, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        c = f * c + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.cell = ConvLSTMCell(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.size()\n",
    "        h = torch.zeros(B, C, H, W, device=x.device)\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        for t in range(T):\n",
    "            h, c = self.cell(x[:, t], h, c)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.mlp(self.avg_pool(x))\n",
    "        max_out = self.mlp(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "        \n",
    "class SpatioTemporalAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(channels)\n",
    "        self.spatial_att = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_att(x)\n",
    "        x = x * self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "class FutureFramePredictorWithAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder()\n",
    "        self.convlstm = ConvLSTM(128, 128)\n",
    "        self.attention = SpatioTemporalAttention(128)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = torch.stack(\n",
    "            [self.encoder(x[:, t]) for t in range(x.size(1))],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        h = self.convlstm(encoded)\n",
    "        h = self.attention(h)   # ðŸ”¥ ATTENTION HERE\n",
    "        return self.decoder(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7769e0af-bc19-4816-bb3d-e3fd8a29b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:   0%|                                                                  | 0/1950 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:   0%|                                                          | 2/1950 [00:01<14:14,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test001] idx=0 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.03025859408080578, 'selected_px': 128, 'selected_ratio': 0.007812499999995231, 'components': 69, 'boxes': 0} err(min=0.000000, max=0.294098, mean=0.008899) score=0.000355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                   | 176/1950 [00:24<04:27,  6.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test002.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  10%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                  | 202/1950 [00:27<03:46,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test002] idx=200 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.017719291150569916, 'selected_px': 30, 'selected_ratio': 0.0018310546874988824, 'components': 20, 'boxes': 0} err(min=0.000000, max=0.394056, mean=0.006983) score=0.000250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                              | 351/1950 [00:47<04:09,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test003.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                            | 402/1950 [00:54<03:18,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test003] idx=400 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.030103126540780067, 'selected_px': 122, 'selected_ratio': 0.007446289062495455, 'components': 71, 'boxes': 0} err(min=0.000001, max=0.323268, mean=0.009732) score=0.000417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                         | 496/1950 [01:06<03:37,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test004.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                      | 602/1950 [01:20<02:58,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test004] idx=600 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.029657650738954544, 'selected_px': 176, 'selected_ratio': 0.010742187499993443, 'components': 42, 'boxes': 1} err(min=0.000000, max=0.305295, mean=0.009055) score=0.000640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                    | 671/1950 [01:29<03:24,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test005.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 802/1950 [01:48<03:53,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test005] idx=800 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 0.9999999403953552, 'heat_mean': 0.04353630170226097, 'selected_px': 337, 'selected_ratio': 0.020568847656237444, 'components': 105, 'boxes': 1} err(min=0.000000, max=0.216815, mean=0.009440) score=0.000409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 816/1950 [01:50<02:56,  6.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test006.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 991/1950 [02:15<02:48,  5.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test007.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                          | 1002/1950 [02:17<02:43,  5.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test007] idx=1000 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.01482929103076458, 'selected_px': 25, 'selected_ratio': 0.0015258789062490687, 'components': 12, 'boxes': 0} err(min=0.000000, max=0.400829, mean=0.005944) score=0.000226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 1166/1950 [02:41<02:02,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test008.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 1202/1950 [02:46<01:34,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test008] idx=1200 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.02584249898791313, 'selected_px': 130, 'selected_ratio': 0.007934570312495157, 'components': 49, 'boxes': 1} err(min=0.000000, max=0.280595, mean=0.007251) score=0.000383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 1341/1950 [03:04<01:32,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test009.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 1402/1950 [03:13<01:12,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test009] idx=1400 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 0.9999999403953552, 'heat_mean': 0.032055556774139404, 'selected_px': 179, 'selected_ratio': 0.010925292968743332, 'components': 77, 'boxes': 0} err(min=0.000000, max=0.245349, mean=0.007865) score=0.000345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 1456/1950 [03:20<01:12,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test010.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 1601/1950 [03:40<00:53,  6.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test011.mp4\n",
      "[Test011] idx=1600 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.03636660426855087, 'selected_px': 203, 'selected_ratio': 0.012390136718742437, 'components': 82, 'boxes': 0} err(min=0.000001, max=0.270470, mean=0.009837) score=0.000435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1776/1950 [04:04<00:31,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing: boxed_videos\\Test012.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1802/1950 [04:08<00:20,  7.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test012] idx=1800 dbg={'thr': 0.35, 'heat_min': 0.0, 'heat_max': 1.0, 'heat_mean': 0.03408050537109375, 'selected_px': 188, 'selected_ratio': 0.011474609374992995, 'components': 77, 'boxes': 0} err(min=0.000001, max=0.257464, mean=0.008775) score=0.000357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Export red-box videos: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1950/1950 [04:27<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. Videos saved in: boxed_videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "test_dataset = UCSDPed2Dataset(\n",
    "    root_dir=\"../datasets/UCSDped2\",\n",
    "    sequence_length=5,\n",
    "    image_size=128,\n",
    "    mode=\"test\"\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model = FutureFramePredictorWithAttention().to(device)\n",
    "model.load_state_dict(torch.load(\"attention_ucsd_ped2.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def normalize_01(x: np.ndarray):\n",
    "    x = x.astype(np.float32)\n",
    "    mn, mx = float(x.min()), float(x.max())\n",
    "    return (x - mn) / (mx - mn + 1e-8)\n",
    "\n",
    "def heatmap_to_boxes_connected_components(\n",
    "    heat_01,         # HxW float in [0,1]\n",
    "    thr=0.35,        # start low to ensure non-empty mask\n",
    "    min_area=20,\n",
    "    keep_top_k=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Uses connected components instead of contours.\n",
    "    More robust when contours are empty due to morphology or thin shapes.\n",
    "    Returns: boxes(list), mask_u8, debug(dict)\n",
    "    \"\"\"\n",
    "    heat_01 = heat_01.astype(np.float32)\n",
    "\n",
    "    # Threshold directly in float space\n",
    "    mask = (heat_01 >= thr).astype(np.uint8)  # 0/1\n",
    "\n",
    "    selected = int(mask.sum())\n",
    "    total = mask.size\n",
    "    selected_ratio = selected / (total + 1e-8)\n",
    "\n",
    "    if selected == 0:\n",
    "        # return empty but with debug\n",
    "        dbg = {\n",
    "            \"thr\": float(thr),\n",
    "            \"heat_min\": float(heat_01.min()),\n",
    "            \"heat_max\": float(heat_01.max()),\n",
    "            \"heat_mean\": float(heat_01.mean()),\n",
    "            \"selected_px\": selected,\n",
    "            \"selected_ratio\": float(selected_ratio),\n",
    "            \"components\": 0,\n",
    "            \"boxes\": 0\n",
    "        }\n",
    "        return [], (mask * 255).astype(np.uint8), dbg\n",
    "\n",
    "    # Connected components on binary mask\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "    # stats: [label, x, y, w, h, area] but actually columns: [x,y,w,h,area]\n",
    "    # label 0 is background; start from 1\n",
    "    boxes = []\n",
    "    for lab in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[lab]\n",
    "        if area >= min_area:\n",
    "            boxes.append((int(x), int(y), int(w), int(h), int(area)))\n",
    "\n",
    "    # Sort by area desc\n",
    "    boxes.sort(key=lambda b: b[4], reverse=True)\n",
    "\n",
    "    # Keep top-k\n",
    "    boxes = boxes[:keep_top_k]\n",
    "\n",
    "    # Drop area from final output\n",
    "    boxes_out = [(x, y, w, h) for (x, y, w, h, area) in boxes]\n",
    "\n",
    "    dbg = {\n",
    "        \"thr\": float(thr),\n",
    "        \"heat_min\": float(heat_01.min()),\n",
    "        \"heat_max\": float(heat_01.max()),\n",
    "        \"heat_mean\": float(heat_01.mean()),\n",
    "        \"selected_px\": selected,\n",
    "        \"selected_ratio\": float(selected_ratio),\n",
    "        \"components\": int(num_labels - 1),\n",
    "        \"boxes\": len(boxes_out)\n",
    "    }\n",
    "\n",
    "    return boxes_out, (mask * 255).astype(np.uint8), dbg\n",
    "\n",
    "# =========================\n",
    "# PARAMETERS (DEBUG-FIRST)\n",
    "# =========================\n",
    "output_root = \"boxed_videos\"\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "fps = 10\n",
    "thr = 0.35          # start low; increase later to make it stricter\n",
    "min_area = 20\n",
    "keep_top_k = 1      # draw only one red box\n",
    "\n",
    "alpha = 0.3\n",
    "beta = 0.7\n",
    "\n",
    "def open_writer_for_video(video_name, frame_w, frame_h):\n",
    "    out_path = os.path.join(output_root, f\"{video_name}.mp4\")\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    return cv2.VideoWriter(out_path, fourcc, fps, (frame_w, frame_h)), out_path\n",
    "\n",
    "# =========================\n",
    "# RUN + EXPORT\n",
    "# =========================\n",
    "writer = None\n",
    "current_video_name = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (inputs, target, video_name, target_frame_name) in enumerate(tqdm(test_loader, desc=\"Export red-box videos\")):\n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # new test folder => new video\n",
    "        if current_video_name != video_name[0]:\n",
    "            if writer is not None:\n",
    "                writer.release()\n",
    "                writer = None\n",
    "\n",
    "            current_video_name = video_name[0]\n",
    "            writer, out_path = open_writer_for_video(current_video_name, 128, 128)\n",
    "            print(f\"\\nWriting: {out_path}\")\n",
    "\n",
    "        pred = model(inputs)\n",
    "\n",
    "        # scalar score (not used for drawing now)\n",
    "        pixel_err = torch.mean((pred - target) ** 2)\n",
    "        feat_pred = model.encoder(pred)\n",
    "        feat_gt = model.encoder(target)\n",
    "        feature_err = torch.mean((feat_pred - feat_gt) ** 2)\n",
    "        score = alpha * pixel_err + beta * feature_err\n",
    "\n",
    "        # localization heatmap from absolute error (NO percentile)\n",
    "        err_map = (pred - target).abs()[0, 0].detach().cpu().numpy()\n",
    "        heat_01 = normalize_01(err_map)\n",
    "\n",
    "        boxes, mask_u8, dbg = heatmap_to_boxes_connected_components(\n",
    "            heat_01,\n",
    "            thr=thr,\n",
    "            min_area=min_area,\n",
    "            keep_top_k=keep_top_k\n",
    "        )\n",
    "\n",
    "        # Print diagnostics occasionally\n",
    "        if idx % 200 == 0:\n",
    "            print(f\"[{current_video_name}] idx={idx} dbg={dbg} \"\n",
    "                  f\"err(min={err_map.min():.6f}, max={err_map.max():.6f}, mean={err_map.mean():.6f}) \"\n",
    "                  f\"score={score.item():.6f}\")\n",
    "\n",
    "        # draw on GT\n",
    "        gt_u8 = (target[0, 0].detach().cpu().numpy() * 255.0).astype(np.uint8)\n",
    "        final = cv2.cvtColor(gt_u8, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # RED box\n",
    "        for (x, y, w, h) in boxes:\n",
    "            cv2.rectangle(final, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        # small debug text so you can confirm\n",
    "        cv2.putText(final, f\"{current_video_name} {target_frame_name[0]}\", (6, 14),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        cv2.putText(final, f\"boxes={len(boxes)} thr={thr}\", (6, 28),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "        writer.write(final)\n",
    "\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "\n",
    "print(f\"\\nDone. Videos saved in: {output_root}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
