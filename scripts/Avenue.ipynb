{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e184615-8927-4ca7-af9e-67acc2b02da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only once to create frames\n",
    "# import os\n",
    "# import re\n",
    "# import cv2\n",
    "\n",
    "# ROOT_DIR = \"../datasets/Avenue\"\n",
    "# OUT_ROOT = os.path.join(ROOT_DIR, \"frames\")\n",
    "\n",
    "# def natural_key(s: str):\n",
    "#     return [int(x) if x.isdigit() else x.lower() for x in re.split(r\"(\\d+)\", s)]\n",
    "\n",
    "# def extract_video_to_frames(video_path, out_dir):\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     if not cap.isOpened():\n",
    "#         cap.release()\n",
    "#         raise RuntimeError(f\"Could not open video: {video_path}\")\n",
    "\n",
    "#     idx = 0\n",
    "#     while True:\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         cv2.imwrite(os.path.join(out_dir, f\"{idx:06d}.jpg\"), frame)\n",
    "#         idx += 1\n",
    "\n",
    "#     cap.release()\n",
    "#     return idx\n",
    "\n",
    "# def collect_videos(video_dir):\n",
    "#     video_paths = []\n",
    "#     for name in sorted(os.listdir(video_dir), key=natural_key):\n",
    "#         p = os.path.join(video_dir, name)\n",
    "#         if os.path.isdir(p):\n",
    "#             for f in sorted(os.listdir(p), key=natural_key):\n",
    "#                 if f.lower().endswith(\".avi\"):\n",
    "#                     video_paths.append(os.path.join(p, f))\n",
    "#         else:\n",
    "#             if name.lower().endswith(\".avi\"):\n",
    "#                 video_paths.append(p)\n",
    "#     return video_paths\n",
    "\n",
    "# def extract_split(split):\n",
    "#     in_dir = os.path.join(ROOT_DIR, split)\n",
    "#     out_dir = os.path.join(OUT_ROOT, split)\n",
    "#     os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "#     videos = collect_videos(in_dir)\n",
    "#     if len(videos) == 0:\n",
    "#         raise ValueError(f\"No .avi files found in: {in_dir}\")\n",
    "\n",
    "#     for vp in videos:\n",
    "#         base = os.path.splitext(os.path.basename(vp))[0]\n",
    "#         out_folder = os.path.join(out_dir, base)\n",
    "#         if os.path.isdir(out_folder) and len(os.listdir(out_folder)) > 0:\n",
    "#             # Skip if already extracted\n",
    "#             continue\n",
    "\n",
    "#         n = extract_video_to_frames(vp, out_folder)\n",
    "#         print(f\"[{split}] {base}: extracted {n} frames\")\n",
    "\n",
    "# extract_split(\"train\")\n",
    "# extract_split(\"test\")\n",
    "\n",
    "# print(f\"Done. Frames saved under: {OUT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd751535-aaa9-43f2-af30-c6545ac719a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames: 1439\n",
      "Anomalous frames: 408\n",
      "First 50 labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def avenue_labels_from_volLabel(mat_path: str):\n",
    "    \"\"\"\n",
    "    Avenue GT: volLabel is (1, T) object array.\n",
    "    Each entry is a (H, W) mask (uint8).\n",
    "    Returns: labels (T,) where 1 means anomalous frame.\n",
    "    \"\"\"\n",
    "    mat = loadmat(mat_path)\n",
    "    v = mat[\"volLabel\"]          # object array (1, T)\n",
    "    v = v.reshape(-1)            # -> (T,)\n",
    "\n",
    "    labels = np.zeros(len(v), dtype=np.int64)\n",
    "    for i in range(len(v)):\n",
    "        mask = np.array(v[i])    # (H, W)\n",
    "        labels[i] = 1 if np.any(mask > 0) else 0\n",
    "\n",
    "    return labels\n",
    "\n",
    "labels = avenue_labels_from_volLabel(\"../datasets/Avenue/test_gt/1_label.mat\")\n",
    "print(\"Frames:\", len(labels))\n",
    "print(\"Anomalous frames:\", labels.sum())\n",
    "print(\"First 50 labels:\", labels[:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d875c39f-2191-43bc-ac3f-7c93f5d3c013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw type: <class 'numpy.ndarray'> dtype: object shape: (1, 1439)\n",
      "unwrap depth 1: object array len = 1439\n",
      "  inner dtype: uint8 shape: (360, 640)\n",
      "\n",
      "final vv dtype: uint8 shape: (360, 640) ndim: 2\n",
      "min: 0 max: 0 unique sample: [0] unique_count: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "mat_path = \"../datasets/Avenue/test_gt/1_label.mat\"\n",
    "mat = loadmat(mat_path)\n",
    "\n",
    "v = mat[\"volLabel\"]\n",
    "print(\"raw type:\", type(v), \"dtype:\", v.dtype, \"shape:\", v.shape)\n",
    "\n",
    "# unwrap possible nesting\n",
    "vv = v\n",
    "depth = 0\n",
    "while isinstance(vv, np.ndarray) and vv.dtype == object:\n",
    "    depth += 1\n",
    "    vv = vv.reshape(-1)\n",
    "    print(f\"unwrap depth {depth}: object array len =\", len(vv))\n",
    "    vv = vv[0]\n",
    "    if isinstance(vv, np.ndarray):\n",
    "        print(\"  inner dtype:\", vv.dtype, \"shape:\", vv.shape)\n",
    "    else:\n",
    "        print(\"  inner type:\", type(vv))\n",
    "\n",
    "vv = np.array(vv)\n",
    "print(\"\\nfinal vv dtype:\", vv.dtype, \"shape:\", vv.shape, \"ndim:\", vv.ndim)\n",
    "\n",
    "# print a few unique values to verify mask nature\n",
    "flat = vv.reshape(-1)\n",
    "print(\"min:\", flat.min(), \"max:\", flat.max(), \"unique sample:\", np.unique(flat)[:10], \"unique_count:\", len(np.unique(flat)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab166d5-f11a-4abf-bafb-e1d08636184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video folders: 21\n",
      "GT .mat files: 21\n",
      "\n",
      "---- Checking first 21 videos ----\n",
      "[WARN] 01: frames=1439, gt=230400  -> aligning to min length\n",
      "01) video=      01  frames= 1439  anomalous=    0  anom%=  0.00\n",
      "[WARN] 02: frames=1211, gt=230400  -> aligning to min length\n",
      "02) video=      02  frames= 1211  anomalous=    0  anom%=  0.00\n",
      "[WARN] 03: frames=923, gt=230400  -> aligning to min length\n",
      "03) video=      03  frames=  923  anomalous=    0  anom%=  0.00\n",
      "[WARN] 04: frames=947, gt=230400  -> aligning to min length\n",
      "04) video=      04  frames=  947  anomalous=    0  anom%=  0.00\n",
      "[WARN] 05: frames=1007, gt=230400  -> aligning to min length\n",
      "05) video=      05  frames= 1007  anomalous=    0  anom%=  0.00\n",
      "[WARN] 06: frames=1283, gt=230400  -> aligning to min length\n",
      "06) video=      06  frames= 1283  anomalous=    0  anom%=  0.00\n",
      "[WARN] 07: frames=605, gt=230400  -> aligning to min length\n",
      "07) video=      07  frames=  605  anomalous=    0  anom%=  0.00\n",
      "[WARN] 08: frames=36, gt=230400  -> aligning to min length\n",
      "08) video=      08  frames=   36  anomalous=    0  anom%=  0.00\n",
      "[WARN] 09: frames=1175, gt=230400  -> aligning to min length\n",
      "09) video=      09  frames= 1175  anomalous=    0  anom%=  0.00\n",
      "[WARN] 10: frames=841, gt=230400  -> aligning to min length\n",
      "10) video=      10  frames=  841  anomalous=    0  anom%=  0.00\n",
      "[WARN] 11: frames=472, gt=230400  -> aligning to min length\n",
      "11) video=      11  frames=  472  anomalous=    0  anom%=  0.00\n",
      "[WARN] 12: frames=1271, gt=230400  -> aligning to min length\n",
      "12) video=      12  frames= 1271  anomalous=    0  anom%=  0.00\n",
      "[WARN] 13: frames=549, gt=230400  -> aligning to min length\n",
      "13) video=      13  frames=  549  anomalous=    0  anom%=  0.00\n",
      "[WARN] 14: frames=507, gt=230400  -> aligning to min length\n",
      "14) video=      14  frames=  507  anomalous=    0  anom%=  0.00\n",
      "[WARN] 15: frames=1001, gt=230400  -> aligning to min length\n",
      "15) video=      15  frames= 1001  anomalous=    0  anom%=  0.00\n",
      "[WARN] 16: frames=740, gt=230400  -> aligning to min length\n",
      "16) video=      16  frames=  740  anomalous=    0  anom%=  0.00\n",
      "[WARN] 17: frames=426, gt=230400  -> aligning to min length\n",
      "17) video=      17  frames=  426  anomalous=    0  anom%=  0.00\n",
      "[WARN] 18: frames=294, gt=230400  -> aligning to min length\n",
      "18) video=      18  frames=  294  anomalous=    0  anom%=  0.00\n",
      "[WARN] 19: frames=248, gt=230400  -> aligning to min length\n",
      "19) video=      19  frames=  248  anomalous=    0  anom%=  0.00\n",
      "[WARN] 20: frames=273, gt=230400  -> aligning to min length\n",
      "20) video=      20  frames=  273  anomalous=    0  anom%=  0.00\n",
      "[WARN] 21: frames=76, gt=230400  -> aligning to min length\n",
      "21) video=      21  frames=   76  anomalous=    0  anom%=  0.00\n",
      "\n",
      "TOTAL frames: 15324\n",
      "TOTAL anomalous frames: 0\n",
      "Overall anom%: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# QUICK GT CHECK (Avenue) - NO TRAINING / NO TESTING\n",
    "# Purpose:\n",
    "# 1) Verify your GT .mat files load correctly (volLabel)\n",
    "# 2) Verify GT length matches extracted frame counts\n",
    "# 3) Print anomaly stats per video\n",
    "#\n",
    "# Expected:\n",
    "#   ../datasets/Avenue/frames/test/<video_folder>/*.jpg\n",
    "#   ../datasets/Avenue/test_gt/1_label.mat, 2_label.mat, ...\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "ROOT_DIR = \"../datasets/Avenue\"\n",
    "FRAMES_TEST_DIR = os.path.join(ROOT_DIR, \"frames\", \"test\")\n",
    "GT_DIR = os.path.join(ROOT_DIR, \"test_gt\")   # contains 1_label.mat, 2_label.mat, ...\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS\n",
    "# ----------------------------\n",
    "def natural_key(s: str):\n",
    "    return [int(x) if x.isdigit() else x.lower() for x in re.split(r\"(\\d+)\", s)]\n",
    "\n",
    "def list_test_videos(frames_test_dir):\n",
    "    vids = sorted(\n",
    "        [d for d in os.listdir(frames_test_dir) if os.path.isdir(os.path.join(frames_test_dir, d))],\n",
    "        key=natural_key\n",
    "    )\n",
    "    if len(vids) == 0:\n",
    "        raise ValueError(f\"No test video folders found in: {frames_test_dir}\")\n",
    "    return vids\n",
    "\n",
    "def count_frames(video_folder_path):\n",
    "    frames = sorted(\n",
    "        [f for f in os.listdir(video_folder_path) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".tif\"))],\n",
    "        key=natural_key\n",
    "    )\n",
    "    return len(frames)\n",
    "\n",
    "def list_gt_matfiles(gt_dir):\n",
    "    mats = sorted(\n",
    "        [f for f in os.listdir(gt_dir) if f.lower().endswith(\".mat\") and \"_label\" in f.lower()],\n",
    "        key=natural_key\n",
    "    )\n",
    "    if len(mats) == 0:\n",
    "        raise ValueError(f\"No *_label.mat files found in: {gt_dir}\")\n",
    "    return [os.path.join(gt_dir, f) for f in mats]\n",
    "\n",
    "def load_volLabel(mat_path):\n",
    "    \"\"\"\n",
    "    Avenue GT commonly has key: 'volLabel'\n",
    "    This function robustly unwraps it into a 1D binary vector.\n",
    "    \"\"\"\n",
    "    mat = loadmat(mat_path)\n",
    "    if \"volLabel\" not in mat:\n",
    "        keys = [k for k in mat.keys() if not k.startswith(\"__\")]\n",
    "        raise KeyError(f\"'volLabel' not found in {mat_path}. Keys={keys}\")\n",
    "\n",
    "    v = mat[\"volLabel\"]\n",
    "\n",
    "    # unwrap MATLAB cell/object nesting\n",
    "    while isinstance(v, np.ndarray) and v.dtype == object:\n",
    "        v = v.reshape(-1)\n",
    "        if len(v) == 0:\n",
    "            break\n",
    "        v = v[0]\n",
    "\n",
    "    v = np.array(v).squeeze()\n",
    "    v = v.reshape(-1)\n",
    "\n",
    "    # binarize\n",
    "    v = (v > 0).astype(np.int64)\n",
    "    return v\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN CHECK\n",
    "# ----------------------------\n",
    "test_videos = list_test_videos(FRAMES_TEST_DIR)\n",
    "gt_files = list_gt_matfiles(GT_DIR)\n",
    "\n",
    "print(\"Test video folders:\", len(test_videos))\n",
    "print(\"GT .mat files:\", len(gt_files))\n",
    "\n",
    "# map by sorted order\n",
    "n = min(len(test_videos), len(gt_files))\n",
    "print(\"\\n---- Checking first\", n, \"videos ----\")\n",
    "\n",
    "total_frames = 0\n",
    "total_anom = 0\n",
    "\n",
    "for i in range(n):\n",
    "    vname = test_videos[i]\n",
    "    vpath = os.path.join(FRAMES_TEST_DIR, vname)\n",
    "    n_frames = count_frames(vpath)\n",
    "\n",
    "    labels = load_volLabel(gt_files[i])\n",
    "\n",
    "    # align lengths (just for checking, no fancy rules)\n",
    "    if len(labels) != n_frames:\n",
    "        print(f\"[WARN] {vname}: frames={n_frames}, gt={len(labels)}  -> aligning to min length\")\n",
    "        m = min(n_frames, len(labels))\n",
    "        labels = labels[:m]\n",
    "        n_frames = m\n",
    "\n",
    "    anom = int(labels.sum())\n",
    "    total_frames += n_frames\n",
    "    total_anom += anom\n",
    "\n",
    "    # print per-video stats\n",
    "    print(f\"{i+1:02d}) video={vname:>8}  frames={n_frames:5d}  anomalous={anom:5d}  anom%={(anom/(n_frames+1e-8))*100:6.2f}\")\n",
    "\n",
    "print(\"\\nTOTAL frames:\", total_frames)\n",
    "print(\"TOTAL anomalous frames:\", total_anom)\n",
    "print(\"Overall anom%:\", (total_anom / (total_frames + 1e-8)) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3158555-ce3a-4d54-b766-d6045a342bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "ROOT_DIR = \"../datasets/Avenue\"\n",
    "FRAMES_DIR = os.path.join(ROOT_DIR, \"frames\")  # created by extraction script\n",
    "SEQUENCE_LENGTH = 5\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 4\n",
    "LR = 1e-4\n",
    "NUM_EPOCHS = 1  # change later (e.g., 30-50) once pipeline is verified\n",
    "MODEL_PATH = \"attention_Avenue.pth\"\n",
    "\n",
    "# Avenue GT folder structure assumed:\n",
    "#   ROOT_DIR/test_gt/1_label/*.mat\n",
    "#   ROOT_DIR/test_gt/2_label/*.mat\n",
    "GT_ROOT = os.path.join(ROOT_DIR, \"test_gt\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ============================================================\n",
    "# HELPERS\n",
    "# ============================================================\n",
    "def natural_key(s: str):\n",
    "    return [int(x) if x.isdigit() else x.lower() for x in re.split(r\"(\\d+)\", s)]\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    return (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET (FRAME FOLDERS) - FAST\n",
    "# Returns:\n",
    "#   input_tensor: (T, 1, H, W)\n",
    "#   target_img:   (1, H, W)\n",
    "#   video_name:   folder name under frames/<split>/\n",
    "#   target_idx:   integer frame index (0-based) inside that folder\n",
    "# ============================================================\n",
    "class AvenueFramesDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length=5, image_size=128, mode=\"train\"):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "\n",
    "        self.video_dir = os.path.join(root_dir, \"frames\", \"train\" if mode == \"train\" else \"test\")\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        self.samples = []\n",
    "        self.video_folders = []\n",
    "        self._prepare_samples()\n",
    "\n",
    "    def _prepare_samples(self):\n",
    "        if not os.path.isdir(self.video_dir):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Frames directory not found: {self.video_dir}\\n\"\n",
    "                f\"Did you extract videos to frames under {FRAMES_DIR}?\"\n",
    "            )\n",
    "\n",
    "        self.video_folders = sorted(\n",
    "            [d for d in os.listdir(self.video_dir) if os.path.isdir(os.path.join(self.video_dir, d))],\n",
    "            key=natural_key\n",
    "        )\n",
    "\n",
    "        if len(self.video_folders) == 0:\n",
    "            raise ValueError(f\"No frame folders found under: {self.video_dir}\")\n",
    "\n",
    "        for video in self.video_folders:\n",
    "            video_path = os.path.join(self.video_dir, video)\n",
    "\n",
    "            frames = sorted([\n",
    "                f for f in os.listdir(video_path)\n",
    "                if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".tif\"))\n",
    "            ], key=natural_key)\n",
    "\n",
    "            if len(frames) <= self.sequence_length:\n",
    "                continue\n",
    "\n",
    "            for start in range(len(frames) - self.sequence_length):\n",
    "                # inputs: start..start+T-1, target: start+T\n",
    "                self.samples.append((video_path, video, frames, start))\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise ValueError(\n",
    "                f\"No samples created. Check that each folder has > {self.sequence_length} frames.\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, video_name, frames, start = self.samples[idx]\n",
    "\n",
    "        input_tensor = []\n",
    "        for t in range(self.sequence_length):\n",
    "            img = Image.open(os.path.join(video_path, frames[start + t])).convert(\"L\")\n",
    "            img = self.transform(img)\n",
    "            input_tensor.append(img)\n",
    "\n",
    "        input_tensor = torch.stack(input_tensor, dim=0)\n",
    "\n",
    "        target_idx = start + self.sequence_length\n",
    "        target_img = Image.open(os.path.join(video_path, frames[target_idx])).convert(\"L\")\n",
    "        target_img = self.transform(target_img)\n",
    "\n",
    "        return input_tensor, target_img, video_name, target_idx\n",
    "\n",
    "# ============================================================\n",
    "# MODEL (same architecture; ConvLSTM init fix included)\n",
    "# ============================================================\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.conv = nn.Conv2d(input_dim + hidden_dim, 4 * hidden_dim, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        combined = torch.cat([x, h], dim=1)\n",
    "        i, f, o, g = torch.split(self.conv(combined), self.hidden_dim, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(i)\n",
    "        f = torch.sigmoid(f)\n",
    "        o = torch.sigmoid(o)\n",
    "        g = torch.tanh(g)\n",
    "\n",
    "        c = f * c + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.cell = ConvLSTMCell(input_dim, hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.size()\n",
    "\n",
    "        # FIX: hidden state channels must be hidden_dim, not C\n",
    "        h = torch.zeros(B, self.hidden_dim, H, W, device=x.device)\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        for t in range(T):\n",
    "            h, c = self.cell(x[:, t], h, c)\n",
    "\n",
    "        return h\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=8):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.mlp(self.avg_pool(x))\n",
    "        max_out = self.mlp(self.max_pool(x))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x))\n",
    "\n",
    "class SpatioTemporalAttention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(channels)\n",
    "        self.spatial_att = SpatialAttention()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channel_att(x)\n",
    "        x = x * self.spatial_att(x)\n",
    "        return x\n",
    "\n",
    "class FutureFramePredictorWithAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder()\n",
    "        self.convlstm = ConvLSTM(128, 128)\n",
    "        self.attention = SpatioTemporalAttention(128)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, 1, H, W)\n",
    "        encoded = torch.stack([self.encoder(x[:, t]) for t in range(x.size(1))], dim=1)\n",
    "        h = self.convlstm(encoded)\n",
    "        h = self.attention(h)\n",
    "        return self.decoder(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e012356-9c54-4305-9827-980cd820485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|███████████████████████████████████████████████████████████████████| 3812/3812 [09:12<00:00,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Avg Loss: 0.003305\n",
      "Saved model -> attention_Avenue.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAIN\n",
    "# ============================================================\n",
    "train_dataset = AvenueFramesDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    mode=\"train\"\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,       # minimal speedup\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "model = FutureFramePredictorWithAttention().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, target, _, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(inputs)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Avg Loss: {running_loss / len(train_loader):.6f}\")\n",
    "\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"Saved model -> {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721431bb-9ada-4195-a288-d335fbf92f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Balu\\AppData\\Local\\Temp\\ipykernel_23432\\834409804.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  attention_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
      "Testing (Pixel+Feature):   0%|                                                               | 0/15219 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TEST: Compute combined anomaly scores + store meta\n",
    "# ============================================================\n",
    "test_dataset = AvenueFramesDataset(\n",
    "    root_dir=ROOT_DIR,\n",
    "    sequence_length=SEQUENCE_LENGTH,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    mode=\"test\"\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "attention_model = FutureFramePredictorWithAttention().to(device)\n",
    "attention_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "attention_model.eval()\n",
    "\n",
    "alpha = 0.3\n",
    "beta = 0.7\n",
    "\n",
    "combined_scores = []\n",
    "meta = []  # (video_name, target_frame_idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, target, video_name, target_idx in tqdm(test_loader, desc=\"Testing (Pixel+Feature)\"):\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        pred = attention_model(inputs)\n",
    "\n",
    "        pixel_err = torch.mean((pred - target) ** 2)\n",
    "        feat_pred = attention_model.encoder(pred)\n",
    "        feat_gt = attention_model.encoder(target)\n",
    "        feature_err = torch.mean((feat_pred - feat_gt) ** 2)\n",
    "\n",
    "        score = alpha * pixel_err + beta * feature_err\n",
    "\n",
    "        combined_scores.append(score.item())\n",
    "        meta.append((video_name[0], int(target_idx.item())))\n",
    "\n",
    "scores = normalize(combined_scores)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(scores)\n",
    "plt.title(\"Normalized Anomaly Scores (Avenue: Attention + Pixel/Feature)\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc99f2a0-f9be-4523-a15a-57bbb5b09ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GT LOADING (Avenue .mat folders) - robust inference\n",
    "# Assumes:\n",
    "#   test_gt/1_label/*.mat corresponds to test video folder order\n",
    "#   test_gt/2_label/*.mat corresponds to next test video folder, etc.\n",
    "# ============================================================\n",
    "def find_first_mat(folder_path: str):\n",
    "    mats = sorted([f for f in os.listdir(folder_path) if f.lower().endswith(\".mat\")], key=natural_key)\n",
    "    if not mats:\n",
    "        return None\n",
    "    return os.path.join(folder_path, mats[0])\n",
    "\n",
    "def to_1d_int_array(x):\n",
    "    x = np.array(x)\n",
    "    if x.dtype == object:\n",
    "        flat = x.reshape(-1)\n",
    "        if len(flat) == 1:\n",
    "            x = np.array(flat[0])\n",
    "        else:\n",
    "            raise ValueError(\"Object array has multiple entries; cannot convert directly to vector.\")\n",
    "    return x.reshape(-1).astype(np.int64)\n",
    "\n",
    "def infer_labels_from_mat(mat_path: str, n_frames: int):\n",
    "    mat = loadmat(mat_path)\n",
    "    keys = [k for k in mat.keys() if not k.startswith(\"__\")]\n",
    "\n",
    "    arrays = [(k, mat[k]) for k in keys if isinstance(mat[k], np.ndarray) and mat[k].size > 1]\n",
    "    if not arrays:\n",
    "        raise ValueError(f\"No usable arrays in {mat_path}. Keys: {keys}\")\n",
    "\n",
    "    def key_rank(k):\n",
    "        lk = k.lower()\n",
    "        prefs = [\"label\", \"labels\", \"gt\", \"ground\", \"truth\", \"mask\"]\n",
    "        return 0 if any(p in lk for p in prefs) else 1\n",
    "\n",
    "    arrays = sorted(arrays, key=lambda kv: (key_rank(kv[0]), -kv[1].size))\n",
    "\n",
    "    for k, v in arrays:\n",
    "        # Case 1: per-frame binary vector length == n_frames\n",
    "        try:\n",
    "            vec = to_1d_int_array(v)\n",
    "            uniq = np.unique(vec)\n",
    "            if set(uniq.tolist()).issubset({0, 1}) and len(vec) == n_frames:\n",
    "                return vec\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Case 2: intervals (K,2) or (2,K)\n",
    "        if isinstance(v, np.ndarray) and v.dtype != object and v.ndim == 2 and (v.shape[1] == 2 or v.shape[0] == 2):\n",
    "            intervals = v\n",
    "            if intervals.shape[1] != 2 and intervals.shape[0] == 2:\n",
    "                intervals = intervals.T\n",
    "            intervals = intervals.astype(np.int64)\n",
    "\n",
    "            labels = np.zeros(n_frames, dtype=np.int64)\n",
    "            starts = intervals[:, 0]\n",
    "            one_based = (np.any(starts == 1) and not np.any(starts == 0))\n",
    "\n",
    "            for s, e in intervals:\n",
    "                if one_based:\n",
    "                    s -= 1\n",
    "                    e -= 1\n",
    "                s0 = max(int(s), 0)\n",
    "                e0 = min(int(e), n_frames - 1)\n",
    "                if e0 >= s0:\n",
    "                    labels[s0:e0 + 1] = 1\n",
    "            return labels\n",
    "\n",
    "        # Case 3: object/cell containing intervals\n",
    "        if isinstance(v, np.ndarray) and v.dtype == object:\n",
    "            flat = v.reshape(-1)\n",
    "            if len(flat) == 1:\n",
    "                entry = np.array(flat[0])\n",
    "                if entry.ndim == 2 and (entry.shape[1] == 2 or entry.shape[0] == 2):\n",
    "                    intervals = entry\n",
    "                    if intervals.shape[1] != 2 and intervals.shape[0] == 2:\n",
    "                        intervals = intervals.T\n",
    "                    intervals = intervals.astype(np.int64)\n",
    "\n",
    "                    labels = np.zeros(n_frames, dtype=np.int64)\n",
    "                    starts = intervals[:, 0]\n",
    "                    one_based = (np.any(starts == 1) and not np.any(starts == 0))\n",
    "\n",
    "                    for s, e in intervals:\n",
    "                        if one_based:\n",
    "                            s -= 1\n",
    "                            e -= 1\n",
    "                        s0 = max(int(s), 0)\n",
    "                        e0 = min(int(e), n_frames - 1)\n",
    "                        if e0 >= s0:\n",
    "                            labels[s0:e0 + 1] = 1\n",
    "                    return labels\n",
    "\n",
    "    raise ValueError(f\"Could not infer labels from {mat_path}. Keys={keys}\")\n",
    "\n",
    "def build_per_video_gt_from_folders(test_video_names, gt_root):\n",
    "    gt_folders = sorted(\n",
    "        [d for d in os.listdir(gt_root) if os.path.isdir(os.path.join(gt_root, d)) and d.endswith(\"_label\")],\n",
    "        key=natural_key\n",
    "    )\n",
    "    if len(gt_folders) < len(test_video_names):\n",
    "        raise ValueError(\n",
    "            f\"Not enough GT folders in {gt_root}: gt={len(gt_folders)} test_videos={len(test_video_names)}\"\n",
    "        )\n",
    "\n",
    "    per_video = {}\n",
    "    for i, vname in enumerate(test_video_names):\n",
    "        video_path = os.path.join(ROOT_DIR, \"frames\", \"test\", vname)\n",
    "        if not os.path.isdir(video_path):\n",
    "            raise FileNotFoundError(f\"Test frame folder missing: {video_path}\")\n",
    "\n",
    "        frames = sorted([f for f in os.listdir(video_path) if f.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".tif\"))], key=natural_key)\n",
    "        n_frames = len(frames)\n",
    "\n",
    "        gt_folder = os.path.join(gt_root, gt_folders[i])\n",
    "        mat_path = find_first_mat(gt_folder)\n",
    "        if mat_path is None:\n",
    "            raise FileNotFoundError(f\"No .mat found in {gt_folder}\")\n",
    "\n",
    "        labels = infer_labels_from_mat(mat_path, n_frames)\n",
    "        per_video[vname] = labels\n",
    "\n",
    "    return per_video\n",
    "\n",
    "# ============================================================\n",
    "# AUC\n",
    "# ============================================================\n",
    "test_video_names = test_dataset.video_folders\n",
    "per_video_gt = build_per_video_gt_from_folders(test_video_names, GT_ROOT)\n",
    "\n",
    "gt_labels = []\n",
    "for (vname, frame_idx) in meta:\n",
    "    labels = per_video_gt[vname]\n",
    "    if frame_idx < 0 or frame_idx >= len(labels):\n",
    "        raise ValueError(f\"Frame idx out of range for {vname}: idx={frame_idx}, len={len(labels)}\")\n",
    "    gt_labels.append(int(labels[frame_idx]))\n",
    "\n",
    "gt_labels = np.array(gt_labels, dtype=np.int64)\n",
    "\n",
    "auc = roc_auc_score(gt_labels, scores)\n",
    "print(f\"\\nAvenue AUC (Attention + Pixel/Feature): {auc:.4f}\")\n",
    "\n",
    "fpr, tpr, _ = roc_curve(gt_labels, scores)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Avenue ROC (Attention + Pixel/Feature)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# OPTIONAL: Visualize a sample prediction (from test set)\n",
    "# ============================================================\n",
    "idx = min(200, len(test_dataset) - 1)\n",
    "with torch.no_grad():\n",
    "    x, y, vname, fidx = test_dataset[idx]\n",
    "    x = x.unsqueeze(0).to(device)\n",
    "    y = y.unsqueeze(0).to(device)\n",
    "\n",
    "    pred = attention_model(x)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Ground Truth\\n{vname} frame {fidx}\")\n",
    "    plt.imshow(y[0, 0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Predicted\")\n",
    "    plt.imshow(pred[0, 0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
